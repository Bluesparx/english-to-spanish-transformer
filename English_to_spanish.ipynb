{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluesparx/english-to-spanish-transformer/blob/main/English_to_spanish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "afR0-ENB7eh9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.optim import Adam\n",
        "from difflib import SequenceMatcher\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                d_model,\n",
        "                ffn_hidden,\n",
        "                num_heads,\n",
        "                drop_prob,\n",
        "                num_layers,\n",
        "                max_sequence_length,\n",
        "                spn_vocab_size,\n",
        "                english_to_index,\n",
        "                spanish_to_index,\n",
        "                START_TOKEN,\n",
        "                END_TOKEN,\n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_ind, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob,num_layers, max_sequence_length, spanish_to_ind, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, spn_vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HtQDw86jQjIf"
      },
      "outputs": [],
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indices = [self.language_to_index.get(token, self.language_to_index[self.PADDING_TOKEN]) for token in sentence]\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "\n",
        "            padding_needed = self.max_sequence_length - len(sentence_word_indices)\n",
        "            if padding_needed > 0:\n",
        "                sentence_word_indices.extend([self.language_to_index[self.PADDING_TOKEN]] * padding_needed)\n",
        "            return sentence_word_indices[:self.max_sequence_length]\n",
        "\n",
        "        tokenized = [tokenize(sentence, start_token, end_token) for sentence in batch]\n",
        "        tokenized = torch.tensor(tokenized, dtype=torch.long)\n",
        "        return tokenized\n",
        "\n",
        "    def forward(self, x, start_token, end_token):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQmSToZn7hLp",
        "outputId": "3db46159-39a3-49f7-ffd1-8ba6d94cdd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lonnieqin/englishspanish-translation-dataset\n",
            "License(s): unknown\n",
            "englishspanish-translation-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "!echo '{\"username\":\"naziahassan00042\",\"key\":\"8eb8a0d58fcb6b7db2b45ae245435c81\"}' > /content/kaggle.json\n",
        "!chmod 600 /content/kaggle.json\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "!kaggle datasets download -d lonnieqin/englishspanish-translation-dataset\n",
        "\n",
        "with zipfile.ZipFile('/content/englishspanish-translation-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')\n",
        "\n",
        "df = pd.read_csv('/content/dataset/data.csv')\n",
        "\n",
        "english_set = df.iloc[:, 0]  # First column for English\n",
        "spanish_set = df.iloc[:, 1]  # Second column for Spanish\n",
        "\n",
        "START_TOKEN = '<start>'\n",
        "PADDING_TOKEN = '<pad>'\n",
        "END_TOKEN = '<end>'\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "                        'Y', 'Z',\n",
        "                        '[', '\\\\', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "spanish_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?',\n",
        "                      '¡', '¿',\n",
        "                      'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "                      'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "                      'Y', 'Z',\n",
        "                      'á', 'é', 'í', 'ó', 'ú', 'ü', 'Á', 'É', 'Í', 'Ó', 'Ú', 'Ü',\n",
        "                      'ñ', 'Ñ',\n",
        "                      '[', '\\\\', ']', '^', '_', '`',\n",
        "                      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                      'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                      'y', 'z',\n",
        "                      '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "ind_to_spanish = {k: v for k, v in enumerate(spanish_vocabulary)}\n",
        "spanish_to_ind = {v: k for k, v in enumerate(spanish_vocabulary)}\n",
        "ind_to_english = {k: v for k, v in enumerate(english_vocabulary)}\n",
        "english_to_ind = {v: k for k, v in enumerate(english_vocabulary)}\n",
        "\n",
        "english_sentences = english_set.tolist()\n",
        "spanish_sentences = spanish_set.tolist()\n",
        "\n",
        "max_sequence_length = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "GbSO1DBwaeOF"
      },
      "outputs": [],
      "source": [
        "# check for valid sentences\n",
        "def isvalid(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def isvalidlength(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 2)  # need to readd eos and start token\n",
        "\n",
        "valid_indices = []\n",
        "for index in range(len(english_sentences)):\n",
        "    spanish_sentence, english_sentence = spanish_sentences[index], english_sentences[index]\n",
        "    if isvalidlength(spanish_sentence, max_sequence_length) \\\n",
        "            and isvalidlength(english_sentence, max_sequence_length) \\\n",
        "            and isvalid(spanish_sentence, spanish_vocabulary) \\\n",
        "            and isvalid(english_sentence, english_vocabulary):\n",
        "        valid_indices.append(index)\n",
        "\n",
        "# Filter sentences based on valid indices\n",
        "english_sentences = [english_sentences[i] for i in valid_indices]\n",
        "spanish_sentences = [spanish_sentences[i] for i in valid_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dfx2mBCvfd-",
        "outputId": "e9bfaba9-d022-4d50-a889-fb1064dc5b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample English sentences: ['Go.', 'Go.', 'Go.', 'Go.', 'Hi.']\n",
            "Sample Spanish sentences: ['Ve.', 'Vete.', 'Vaya.', 'Váyase.', 'Hola.']\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample English sentences:\", english_sentences[:5])\n",
        "print(\"Sample Spanish sentences:\", spanish_sentences[:5])\n",
        "\n",
        "max_sentences = 1500\n",
        "english_sentences = english_sentences[:max_sentences]\n",
        "spanish_sentences = spanish_sentences[:max_sentences]\n",
        "\n",
        "assert len(english_sentences) == len(spanish_sentences), \"mismatch\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "J7B44ZwWanRE"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "batch_size = 15\n",
        "ffn_hidden = 1024\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "spn_vocab_size = len(spanish_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          spn_vocab_size,\n",
        "                          english_to_ind,\n",
        "                          spanish_to_ind,\n",
        "                          START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "\n",
        "sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, english_to_ind , START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, spanish_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.spanish_sentences = spanish_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.spanish_sentences[idx]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=spanish_to_ind[PADDING_TOKEN], reduction='none')\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optim, step_size=1, gamma=0.9)  # Decay LR by 10% every epoch\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "dataset = TextDataset(english_sentences, spanish_sentences)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "# Move model to device\n",
        "transformer.to(device)\n",
        "\n",
        "clip_value = 1.0\n",
        "\n",
        "# Initialize parameters\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IMA1QW3APrw",
        "outputId": "defbac06-752e-4f99-d466-f389bcda122c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 15\n",
            "Epoch 0\n",
            "Iteration 0 : 270.95709228515625\n",
            "English: She walks.\n",
            "Spanish Translation: Anda.\n",
            "Spanish Prediction: Eom s esteró.\n",
            "Similarity: 0.47458331579021235\n",
            "Iteration 50 : 278.3660888671875\n",
            "English: Let Tom go.\n",
            "Spanish Translation: Deja ir a Tom.\n",
            "Spanish Prediction: ¡íguelo.\n",
            "Similarity: 0.4678786805359512\n",
            "Train Accuracy for Epoch 0: 0.4650376446805105\n",
            "Epoch 1\n",
            "Iteration 0 : 262.16607666015625\n",
            "English: Help!\n",
            "Spanish Translation: ¡Auxilio!\n",
            "Spanish Prediction: Eire a  a.\n",
            "Similarity: 0.5731641343406049\n",
            "Iteration 50 : 288.65997314453125\n",
            "English: Go for it.\n",
            "Spanish Translation: Ve a por ello.\n",
            "Spanish Prediction: Eo pago.óeosa.\n",
            "Similarity: 0.4986527031112982\n",
            "Train Accuracy for Epoch 1: 0.5080774063625481\n",
            "Epoch 2\n",
            "Iteration 0 : 255.5123748779297\n",
            "English: So long.\n",
            "Spanish Translation: Hasta la vista.\n",
            "Spanish Prediction: Eesuntianes.\n",
            "Similarity: 0.5946393517143281\n",
            "Iteration 50 : 336.3207702636719\n",
            "English: Tom paused.\n",
            "Spanish Translation: Tom se detuvo.\n",
            "Spanish Prediction: Es eíeaoe\n",
            "Similarity: 0.5025111517241876\n",
            "Train Accuracy for Epoch 2: 0.5127011400137585\n",
            "Epoch 3\n",
            "Iteration 0 : 283.5024719238281\n",
            "English: I'll pay.\n",
            "Spanish Translation: Yo pagaré.\n",
            "Spanish Prediction: ¡ Tom la asprr.a.a.\n",
            "Similarity: 0.43773153014512417\n",
            "Iteration 50 : 332.1456298828125\n",
            "English: I'm better.\n",
            "Spanish Translation: Yo soy mejor.\n",
            "Spanish Prediction: Tstoy eojaarts\n",
            "Similarity: 0.5117756343107266\n",
            "Train Accuracy for Epoch 3: 0.5084894640011346\n",
            "Epoch 4\n",
            "Iteration 0 : 221.95684814453125\n",
            "English: Is it far?\n",
            "Spanish Translation: ¿Está lejos?\n",
            "Spanish Prediction: EEenne dalor aom?\n",
            "Similarity: 0.520519370699745\n",
            "Iteration 50 : 264.8233642578125\n",
            "English: I agreed.\n",
            "Spanish Translation: Accedí.\n",
            "Spanish Prediction: Elv lsto.\n",
            "Similarity: 0.5135369543971712\n",
            "Train Accuracy for Epoch 4: 0.5142070236442907\n",
            "Epoch 5\n",
            "Iteration 0 : 241.45843505859375\n",
            "English: It's new.\n",
            "Spanish Translation: Es nuevo.\n",
            "Spanish Prediction: Te tstoy cetierto.\n",
            "Similarity: 0.44927982151017687\n",
            "Iteration 50 : 258.6192626953125\n",
            "English: I'm so fat.\n",
            "Spanish Translation: Estoy tan gorda.\n",
            "Spanish Prediction: Ebíre e a Tom.\n",
            "Similarity: 0.5124585252633604\n",
            "Train Accuracy for Epoch 5: 0.5101921231273082\n",
            "Epoch 6\n",
            "Iteration 0 : 261.07647705078125\n",
            "English: Go ahead!\n",
            "Spanish Translation: Adelante.\n",
            "Spanish Prediction: TDueno \n",
            "Similarity: 0.538438519979554\n",
            "Iteration 50 : 284.34808349609375\n",
            "English: Keep still.\n",
            "Spanish Translation: Manténgase quieto.\n",
            "Spanish Prediction: Ee auerdn.\n",
            "Similarity: 0.49900393121226994\n",
            "Train Accuracy for Epoch 6: 0.5056941458014971\n",
            "Epoch 7\n",
            "Iteration 0 : 264.11749267578125\n",
            "English: I cringed.\n",
            "Spanish Translation: Sentí vergüenza.\n",
            "Spanish Prediction: ¡oy earaer\n",
            "Similarity: 0.4816606750662479\n",
            "Iteration 50 : 246.11904907226562\n",
            "English: It's my CD.\n",
            "Spanish Translation: Es mi CD.\n",
            "Spanish Prediction: Ee sar.\n",
            "Similarity: 0.5220195047395434\n",
            "Train Accuracy for Epoch 7: 0.520622968978462\n",
            "Epoch 8\n",
            "Iteration 0 : 287.8559875488281\n",
            "English: Stop it.\n",
            "Spanish Translation: Deténlo.\n",
            "Spanish Prediction: Eraciam \n",
            "Similarity: 0.4678096828514112\n",
            "Iteration 50 : 265.0248107910156\n",
            "English: Have faith.\n",
            "Spanish Translation: Tengan fe.\n",
            "Spanish Prediction: Ténntete.c aui.o.e.euiero.\n",
            "Similarity: 0.5213596424334986\n",
            "Train Accuracy for Epoch 8: 0.5205167748489257\n",
            "Epoch 9\n",
            "Iteration 0 : 228.89425659179688\n",
            "English: Tom smiled.\n",
            "Spanish Translation: Tom sonrió.\n",
            "Spanish Prediction: Eeseóó. aqaalo.\n",
            "Similarity: 0.5287137882939982\n",
            "Iteration 50 : 318.97296142578125\n",
            "English: I'm fat.\n",
            "Spanish Translation: Soy gorda.\n",
            "Spanish Prediction: TNo conoddo!\n",
            "Similarity: 0.5323527747645567\n",
            "Train Accuracy for Epoch 9: 0.5322752741233301\n",
            "Epoch 10\n",
            "Iteration 0 : 223.61038208007812\n",
            "English: Here he is!\n",
            "Spanish Translation: ¡Aquí está él!\n",
            "Spanish Prediction: Eoremos.erobaro\n",
            "Similarity: 0.5516485592549666\n",
            "Iteration 50 : 263.6546630859375\n",
            "English: Tom won.\n",
            "Spanish Translation: Tom ganó.\n",
            "Spanish Prediction: Tl es eient.\n",
            "Similarity: 0.5395946714718827\n",
            "Train Accuracy for Epoch 10: 0.5407157645504462\n",
            "Epoch 11\n",
            "Iteration 0 : 212.9549560546875\n",
            "English: Who's Tom?\n",
            "Spanish Translation: ¿Quién es Tom?\n",
            "Spanish Prediction: Tete a ear asoo.\n",
            "Similarity: 0.6129241345292912\n",
            "Iteration 50 : 301.87957763671875\n",
            "English: I'm single.\n",
            "Spanish Translation: Soy soltero.\n",
            "Spanish Prediction: Eérntate.a catcacaaevos.\n",
            "Similarity: 0.5396560165036188\n",
            "Train Accuracy for Epoch 11: 0.5489096397329863\n",
            "Epoch 12\n",
            "Iteration 0 : 214.7871551513672\n",
            "English: Is it here?\n",
            "Spanish Translation: ¿Aquí?\n",
            "Spanish Prediction: EPoámanlo T Tomás!\n",
            "Similarity: 0.5556884966038282\n",
            "Iteration 50 : 262.6436767578125\n",
            "English: Let me go!\n",
            "Spanish Translation: ¡Déjame marchar!\n",
            "Spanish Prediction: Eso eo terfen..e.\n",
            "Similarity: 0.5329067957046218\n",
            "Train Accuracy for Epoch 12: 0.5363016153443795\n",
            "Epoch 13\n",
            "Iteration 0 : 269.7359619140625\n",
            "English: Who ran?\n",
            "Spanish Translation: ¿Quién corría?\n",
            "Spanish Prediction: Es ma bord.\n",
            "Similarity: 0.5651091433545795\n",
            "Iteration 50 : 246.52505493164062\n",
            "English: He coughed.\n",
            "Spanish Translation: Tosió.\n",
            "Spanish Prediction: El es aaltnt...\n",
            "Similarity: 0.5343858546303516\n",
            "Train Accuracy for Epoch 13: 0.5362263865499785\n",
            "Epoch 14\n",
            "Iteration 0 : 242.94097900390625\n",
            "English: Don't yell.\n",
            "Spanish Translation: No grites.\n",
            "Spanish Prediction: TDira ca io cdras!\n",
            "Similarity: 0.5332065332864294\n",
            "Iteration 50 : 262.0797424316406\n",
            "English: Go now.\n",
            "Spanish Translation: Ve ahora mismo.\n",
            "Spanish Prediction: Eom ee muinó.lod ad ento.\n",
            "Similarity: 0.5356210816345399\n",
            "Train Accuracy for Epoch 14: 0.5309715080790162\n",
            "Epoch 15\n",
            "Iteration 0 : 254.78387451171875\n",
            "English: Sit down!\n",
            "Spanish Translation: ¡Sentate!\n",
            "Spanish Prediction: Vos sras enpartmos.\n",
            "Similarity: 0.542535836769583\n",
            "Iteration 50 : 263.87799072265625\n",
            "English: Get down.\n",
            "Spanish Translation: Túmbate.\n",
            "Spanish Prediction: Tslsrnnienleena.\n",
            "Similarity: 0.5380275060274715\n",
            "Train Accuracy for Epoch 15: 0.5385245092904956\n",
            "Epoch 16\n",
            "Iteration 0 : 238.23068237304688\n",
            "English: Tom yawned.\n",
            "Spanish Translation: Tom bostezó.\n",
            "Spanish Prediction: Ee erioona Tom.\n",
            "Similarity: 0.5114539647148343\n",
            "Iteration 50 : 269.4081726074219\n",
            "English: Tom obeyed.\n",
            "Spanish Translation: Tom obedeció.\n",
            "Spanish Prediction: Ee llra. .aa.\n",
            "Similarity: 0.5488731078563338\n",
            "Train Accuracy for Epoch 16: 0.5402389545404707\n",
            "Epoch 17\n",
            "Iteration 0 : 248.9601287841797\n",
            "English: All aboard!\n",
            "Spanish Translation: ¡Todos a bordo!\n",
            "Spanish Prediction: Tejame.aera\n",
            "Similarity: 0.5164648973317704\n",
            "Iteration 50 : 248.73907470703125\n",
            "English: Tom moaned.\n",
            "Spanish Translation: Tom gimió.\n",
            "Spanish Prediction: Tire aacoa a elante.\n",
            "Similarity: 0.5334259906446848\n",
            "Train Accuracy for Epoch 17: 0.5364273775751108\n",
            "Epoch 18\n",
            "Iteration 0 : 251.47988891601562\n",
            "English: I moved.\n",
            "Spanish Translation: Me mudé.\n",
            "Spanish Prediction: Ete a te \n",
            "Similarity: 0.5018598200951143\n",
            "Iteration 50 : 329.5240783691406\n",
            "English: Be serious.\n",
            "Spanish Translation: Sé serio.\n",
            "Spanish Prediction: Eespuerede aat .eos.\n",
            "Similarity: 0.5352449814665392\n",
            "Train Accuracy for Epoch 18: 0.5379524330878488\n",
            "Epoch 19\n",
            "Iteration 0 : 210.25674438476562\n",
            "English: Seize him!\n",
            "Spanish Translation: ¡A por él!\n",
            "Spanish Prediction: EVáral\n",
            "Similarity: 0.5623576005958305\n",
            "Iteration 50 : 276.5517883300781\n",
            "English: Got it?\n",
            "Spanish Translation: ¿Entendiste?\n",
            "Spanish Prediction: Eom eoo ó...\n",
            "Similarity: 0.5392796534852213\n",
            "Train Accuracy for Epoch 19: 0.540148537105428\n",
            "Train Accuracy: 0.5242115345718963\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists for reference and candidate translations\n",
        "references = []\n",
        "candidates = []\n",
        "\n",
        "iterator = iter(train_loader)\n",
        "\n",
        "\n",
        "print(f\"batch size: {batch_size}\")\n",
        "\n",
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 20\n",
        "train_accuracy = 0\n",
        "final_accuracy = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    # Clear references and candidates for each epoch\n",
        "    references.clear()\n",
        "    candidates.clear()\n",
        "\n",
        "    total_accuracy = 0  # Initialize total accuracy for the epoch\n",
        "    total_batches = 0  # Initialize batch counter for the epoch\n",
        "\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        similarity = 0\n",
        "        transformer.train()\n",
        "        eng_batch, spn_batch = batch\n",
        "\n",
        "        mask = torch.full([max_sequence_length, max_sequence_length], float('-inf'))\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        encoder_self_attention_mask = mask\n",
        "        decoder_self_attention_mask = mask\n",
        "        decoder_cross_attention_mask = mask\n",
        "\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predictions\n",
        "        spn_predictions = transformer(eng_batch,\n",
        "                                      spn_batch,\n",
        "                                      encoder_self_attention_mask.to(device),\n",
        "                                      decoder_self_attention_mask.to(device),\n",
        "                                      decoder_cross_attention_mask.to(device),\n",
        "                                      enc_start_token=True,\n",
        "                                      enc_end_token=True,\n",
        "                                      dec_start_token=True,\n",
        "                                      dec_end_token=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(spn_batch, start_token=False, end_token=True)\n",
        "        loss = criterion(spn_predictions.view(-1, spn_vocab_size).to(device),\n",
        "                         labels.view(-1).to(device))\n",
        "\n",
        "        # Ignore padding\n",
        "        valid_indices = torch.where(labels.view(-1) != spanish_to_ind[PADDING_TOKEN], True, False)\n",
        "        loss = loss * valid_indices.float()\n",
        "\n",
        "        # Compute the sum of valid losses\n",
        "        valid_loss = loss.sum()\n",
        "\n",
        "        valid_loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), clip_value)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        spn_sentence_predicted = torch.argmax(spn_predictions, axis=2)\n",
        "\n",
        "        for i in range(len(spn_batch)):\n",
        "            reference = spn_batch[i]  # Keep as string\n",
        "            candidate = spn_sentence_predicted[i].tolist()  # Convert tensor to list of indices\n",
        "            # Remove padding and end tokens\n",
        "            candidate = [ind_to_spanish[idx] for idx in candidate if idx not in [spanish_to_ind[PADDING_TOKEN], spanish_to_ind[END_TOKEN]]]\n",
        "            references.append(reference)\n",
        "            candidates.append(candidate)\n",
        "\n",
        "        for cand, ref in zip(candidates, references):\n",
        "            similarity+= similar(ref, cand)\n",
        "\n",
        "        similarity /= len(candidates)\n",
        "        total_loss += valid_loss.item()\n",
        "        batch_accuracy = similarity  # Similarity represents batch accuracy here\n",
        "        total_accuracy += batch_accuracy  # Accumulate batch accuracy\n",
        "        total_batches += 1  # Increment batch counter\n",
        "\n",
        "        if batch_num % 50 == 0:\n",
        "            print(f\"Iteration {batch_num} : {valid_loss.item()}\")\n",
        "            print(f\"English: {eng_batch[len(eng_batch)-1]}\")\n",
        "            print(f\"Spanish Translation: {spn_batch[len(spn_batch)-1]}\")\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in spn_sentence_predicted[0]:\n",
        "                if idx == spanish_to_ind[END_TOKEN]:\n",
        "                    break\n",
        "                predicted_sentence += ind_to_spanish[idx.item()]\n",
        "            print(f\"Spanish Prediction: {predicted_sentence}\")\n",
        "            print(f\"Similarity: {similarity}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Compute average training accuracy for the epoch\n",
        "    train_accuracy = total_accuracy / total_batches\n",
        "    print(f\"Train Accuracy for Epoch {epoch}: {train_accuracy}\")\n",
        "\n",
        "    final_accuracy += train_accuracy\n",
        "\n",
        "print(f\"Train Accuracy: {final_accuracy / num_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "VplfcwZyUgUi"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "torch.save(transformer.state_dict(), 'transformer_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXoKQ2SlsSVG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAIgqHpEcdzU8QwJ/P5wkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}